{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:14:58.908907Z",
     "start_time": "2020-10-13T14:14:58.902509Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, optimizers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# tf.config.list_physical_devices(device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现。\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "# tensorflow 如何设置在GPU上能够复现结果还不太清楚怎么弄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "training_env_seed = 123\n",
    "lr = 1e-4\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(keras.Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.affine1 = layers.Dense(128)\n",
    "        self.dropout = layers.Dropout(rate=0.6)\n",
    "        self.affine2 = layers.Dense(self.action_dim)\n",
    "        \n",
    "    def call(self, obs, training=None):\n",
    "        x = self.affine1(obs)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        action_logits = self.affine2(x)\n",
    "        actions = tf.nn.softmax(action_logits, axis=-1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    def __init__(self, env_name=None, policy=policy, eval_mode=False):\n",
    "        \n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.env.seed(training_env_seed)\n",
    "        \n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        self.policy = policy(self.action_dim)\n",
    "        \n",
    "        self.optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        self.eval_mode = eval_mode\n",
    "        \n",
    "        self.log_probs = []        # 用来记录每个时刻t的log(pi(a_t|s_t))\n",
    "        self.rewards = []          # 用来记录每个时刻t的reward，r_t\n",
    "        self.returns = []          # 用来记录每个时刻t的return，G_t\n",
    "        self.loss = []             # 用来记录每个时刻t的loss：G_t * log(pi(a_t|s_t))\n",
    "        \n",
    "        self.eps = np.finfo(np.float32).eps.item()   # 创建一个很小的浮点数，加在分母，防止0的出现，直接写1e-10也行\n",
    "        \n",
    "    def get_action(self, obs, training=None):\n",
    "        obs = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), axis=0)   # [1, obs_dim]\n",
    "        probs = self.policy(obs, training=training)      # 产生策略函数，是一个关于action的概率\n",
    "        m = tfp.distributions.Categorical(probs=probs)   # 生成一个Categorical分布，在CartPole里是二项分布\n",
    "        action = m.sample()                              # 从分布里采样，采出的是索引\n",
    "        self.log_probs.append(m.log_probs(action))       # 把对应的log概率记录下来, 因为后面导数是对logπ（θ）来求的\n",
    "        \n",
    "        return action.numpy()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:21:33.320679Z",
     "start_time": "2020-10-13T14:21:33.272139Z"
    }
   },
   "outputs": [],
   "source": [
    "m = tfp.distributions.Categorical(probs=[0.1, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
