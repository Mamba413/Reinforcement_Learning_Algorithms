{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:17.758829Z",
     "start_time": "2020-10-14T12:50:13.451877Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from itertools import count\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, optimizers\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# tf.config.list_physical_devices(device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:17.765958Z",
     "start_time": "2020-10-14T12:50:17.761279Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现。\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "# tensorflow 如何设置在GPU上能够复现结果还不太清楚怎么弄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:17.773216Z",
     "start_time": "2020-10-14T12:50:17.769082Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "training_env_seed = 123\n",
    "lr = 1e-4\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:17.786246Z",
     "start_time": "2020-10-14T12:50:17.776864Z"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(keras.Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.affine1 = layers.Dense(128)\n",
    "        self.dropout = layers.Dropout(rate=0.6)\n",
    "        self.affine2 = layers.Dense(self.action_dim)\n",
    "        \n",
    "    def call(self, obs, training=None):\n",
    "        x = self.affine1(obs)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        action_logits = self.affine2(x)\n",
    "        actions = tf.nn.softmax(action_logits, axis=-1)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow和pytorch在写法上会有不同，如果按照pytorch相同的写法，会导致在计算梯度：tape.gradient(loss, self.policy.trainable_variables)时获取不到相应的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:17.812608Z",
     "start_time": "2020-10-14T12:50:17.788865Z"
    }
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    def __init__(self, env_name=None, policy=Policy, eval_mode=False):\n",
    "        \n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.env.seed(training_env_seed)\n",
    "        \n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        self.policy = policy(self.action_dim)\n",
    "        \n",
    "        self.optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        self.eval_mode = eval_mode\n",
    "        \n",
    "        self.log_probs = []        # 用来记录每个时刻t的log(pi(a_t|s_t))\n",
    "        self.rewards = []          # 用来记录每个时刻t的reward，r_t\n",
    "        self.returns = []          # 用来记录每个时刻t的return，G_t\n",
    "        self.loss = []             # 用来记录每个时刻t的loss：G_t * log(pi(a_t|s_t))\n",
    "        \n",
    "        self.eps = np.finfo(np.float32).eps.item()   # 创建一个很小的浮点数，加在分母，防止0的出现，直接写1e-10也行\n",
    "        \n",
    "    def get_action(self, obs, training=None):\n",
    "        obs = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), axis=0)   # [1, obs_dim]\n",
    "        probs = self.policy(obs, training=training)      # 产生策略函数，是一个关于action的概率\n",
    "        m = tfp.distributions.Categorical(probs=probs)   # 生成一个Categorical分布，在CartPole里是二项分布\n",
    "        action = m.sample()                              # 从分布里采样，采出的是索引\n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        R = 0\n",
    "        # policy gradient update\n",
    "        for r in self.rewards[::-1]:     # 倒序\n",
    "            R = r + gamma * R            # 计算t到T的reward折现和\n",
    "            self.returns.insert(0, R)    # 在最前面插入\n",
    "\n",
    "        returns = tf.constant(self.returns, dtype=tf.float32)\n",
    "        returns = (returns - tf.reduce_mean(returns)) / (tf.math.reduce_std(returns) + self.eps)  # 把returns做一个标准化，这样对于action的矫正会更有效果一些，毕竟一条成功的轨迹里并不一定所有的action都是好的\n",
    "\n",
    "        for log_prob, R in zip(self.log_probs, returns):\n",
    "            self.loss.append(-log_prob * R)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.reduce_sum(tf.stack(self.loss, axis=-1))\n",
    "            \n",
    "        self.grads = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        print(self.grads)\n",
    "        self.optimizer.apply_gradients(zip(self.grads, self.policy.trainable_variables))\n",
    "        \n",
    "        del self.rewards[:]    # 把列表清空，但列表还在，[]\n",
    "        del self.returns[:]\n",
    "        del self.log_probs[:]\n",
    "        del self.loss[:]\n",
    "        \n",
    "    \n",
    "    def eval_(self, env, n_trajs):\n",
    "        returns = []\n",
    "        \n",
    "        for i in range(n_trajs):\n",
    "            ep_return = 0\n",
    "            obs = env.reset()\n",
    "            \n",
    "            for step in range(10000):\n",
    "                action = self.get_action(obs, training=False)\n",
    "                obs, reward, done, _ = env.step(action)\n",
    "                ep_return += reward\n",
    "                \n",
    "                if done:\n",
    "                    returns.append(ep_return)\n",
    "                    break\n",
    "        return np.array(returns).mean()\n",
    "    \n",
    "    \n",
    "    def render(self, env):\n",
    "        obs = env.reset()\n",
    "        for _ in range(10000):\n",
    "            env.render()\n",
    "            action = self.get_action(obs, training=False)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def save(self, step):\n",
    "        self.policy.save_weights('./reinforce_{}.ckpt'.format(step))\n",
    "        \n",
    "    def load(self, path):\n",
    "        if os.path.isfile(path):\n",
    "            self.policy.load_weights(path)\n",
    "        else:\n",
    "            print('No \"{}\" exits for loading'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainging Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T12:50:18.465654Z",
     "start_time": "2020-10-14T12:50:17.814900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['policy/dense/kernel:0', 'policy/dense/bias:0', 'policy/dense_1/kernel:0', 'policy/dense_1/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03ee2ef9d4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0magent_reinforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0735abc5c59f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# 把列表清空，但列表还在，[]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3.6.8/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3.6.8/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1039\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1040\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     logging.warning(\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['policy/dense/kernel:0', 'policy/dense/bias:0', 'policy/dense_1/kernel:0', 'policy/dense_1/bias:0']."
     ]
    }
   ],
   "source": [
    "name = 'CartPole-v0'\n",
    "env_eval = gym.make(name)\n",
    "env_eval.seed(seed)\n",
    "\n",
    "\n",
    "start = timer()\n",
    "running_returns = []\n",
    "agent_reinforce = REINFORCEAgent(env_name=name, policy=Policy)\n",
    "\n",
    "for episode in count(1): # 一直加1的while, 表示一条episode\n",
    "    # print('episode%d'%episode)\n",
    "    obs, ep_return = agent_reinforce.env.reset(), 0\n",
    "    for step in range(10000):\n",
    "        action = agent_reinforce.get_action(obs)\n",
    "        obs, reward, done, _ = agent_reinforce.env.step(action)\n",
    "        agent_reinforce.rewards.append(reward)\n",
    "        ep_return += reward\n",
    "        if done:\n",
    "            running_returns.append(ep_return)\n",
    "            break\n",
    "    \n",
    "    agent_reinforce.train()\n",
    "    \n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(pd.Series(running_returns).rolling(100, 20).mean())\n",
    "        plt.title('episide:{}, time:{}, returns'.format(episode, timedelta(seconds=int(timer()-start))))\n",
    "        plt.show()\n",
    "    if np.array(running_returns)[-10:].mean() > 195:\n",
    "        eval_return = agent_reinforce.eval_(env_eval, 100)\n",
    "        if eval_return > 195:\n",
    "            print(\"Solved! eval return is now {}!\".format(eval_return))\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T14:14:32.398799Z",
     "start_time": "2020-10-14T14:14:32.392978Z"
    }
   },
   "outputs": [],
   "source": [
    "class model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(30,activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(30,activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = tf.convert_to_tensor(input_data)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T14:15:41.115681Z",
     "start_time": "2020-10-14T14:15:41.104318Z"
    }
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, model=model):\n",
    "        self.model = model()\n",
    "        self.gamma = 0.9\n",
    "        self.opt = optimizers.Adam(1e-4)\n",
    "    def act(self,state):\n",
    "        prob = self.model(np.array([state]))\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "\n",
    "    def train(self, states, rewards, actions):\n",
    "        sum_reward = 0\n",
    "        discnt_rewards = []\n",
    "        rewards.reverse()\n",
    "        for r in rewards:\n",
    "            sum_reward = r + self.gamma*sum_reward\n",
    "            discnt_rewards.append(sum_reward)\n",
    "        discnt_rewards.reverse()  \n",
    "\n",
    "        for state, reward, action in zip(states, discnt_rewards, actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                p = self.model(np.array([state]), training=True)\n",
    "                loss = self.a_loss(p, action, reward)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            \n",
    "    \n",
    "    def a_loss(self,prob, action, reward): \n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob*reward\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T14:17:25.108240Z",
     "start_time": "2020-10-14T14:15:41.296575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "total reward after 0 steps is 46.0\n",
      "total reward after 1 steps is 31.0\n",
      "total reward after 2 steps is 48.0\n",
      "total reward after 3 steps is 10.0\n",
      "total reward after 4 steps is 36.0\n",
      "total reward after 5 steps is 40.0\n",
      "total reward after 6 steps is 18.0\n",
      "total reward after 7 steps is 11.0\n",
      "total reward after 8 steps is 11.0\n",
      "total reward after 9 steps is 12.0\n",
      "total reward after 10 steps is 20.0\n",
      "total reward after 11 steps is 44.0\n",
      "total reward after 12 steps is 17.0\n",
      "total reward after 13 steps is 18.0\n",
      "total reward after 14 steps is 26.0\n",
      "total reward after 15 steps is 21.0\n",
      "total reward after 16 steps is 10.0\n",
      "total reward after 17 steps is 10.0\n",
      "total reward after 18 steps is 49.0\n",
      "total reward after 19 steps is 9.0\n",
      "total reward after 20 steps is 14.0\n",
      "total reward after 21 steps is 26.0\n",
      "total reward after 22 steps is 19.0\n",
      "total reward after 23 steps is 25.0\n",
      "total reward after 24 steps is 19.0\n",
      "total reward after 25 steps is 21.0\n",
      "total reward after 26 steps is 12.0\n",
      "total reward after 27 steps is 18.0\n",
      "total reward after 28 steps is 11.0\n",
      "total reward after 29 steps is 46.0\n",
      "total reward after 30 steps is 20.0\n",
      "total reward after 31 steps is 22.0\n",
      "total reward after 32 steps is 18.0\n",
      "total reward after 33 steps is 17.0\n",
      "total reward after 34 steps is 10.0\n",
      "total reward after 35 steps is 10.0\n",
      "total reward after 36 steps is 24.0\n",
      "total reward after 37 steps is 35.0\n",
      "total reward after 38 steps is 22.0\n",
      "total reward after 39 steps is 16.0\n",
      "total reward after 40 steps is 16.0\n",
      "total reward after 41 steps is 17.0\n",
      "total reward after 42 steps is 10.0\n",
      "total reward after 43 steps is 12.0\n",
      "total reward after 44 steps is 12.0\n",
      "total reward after 45 steps is 26.0\n",
      "total reward after 46 steps is 10.0\n",
      "total reward after 47 steps is 15.0\n",
      "total reward after 48 steps is 11.0\n",
      "total reward after 49 steps is 12.0\n",
      "total reward after 50 steps is 14.0\n",
      "total reward after 51 steps is 15.0\n",
      "total reward after 52 steps is 41.0\n",
      "total reward after 53 steps is 17.0\n",
      "total reward after 54 steps is 38.0\n",
      "total reward after 55 steps is 34.0\n",
      "total reward after 56 steps is 9.0\n",
      "total reward after 57 steps is 14.0\n",
      "total reward after 58 steps is 49.0\n",
      "total reward after 59 steps is 12.0\n",
      "total reward after 60 steps is 13.0\n",
      "total reward after 61 steps is 13.0\n",
      "total reward after 62 steps is 31.0\n",
      "total reward after 63 steps is 10.0\n",
      "total reward after 64 steps is 42.0\n",
      "total reward after 65 steps is 33.0\n",
      "total reward after 66 steps is 36.0\n",
      "total reward after 67 steps is 18.0\n",
      "total reward after 68 steps is 13.0\n",
      "total reward after 69 steps is 11.0\n",
      "total reward after 70 steps is 24.0\n",
      "total reward after 71 steps is 24.0\n",
      "total reward after 72 steps is 15.0\n",
      "total reward after 73 steps is 18.0\n",
      "total reward after 74 steps is 53.0\n",
      "total reward after 75 steps is 27.0\n",
      "total reward after 76 steps is 22.0\n",
      "total reward after 77 steps is 28.0\n",
      "total reward after 78 steps is 12.0\n",
      "total reward after 79 steps is 9.0\n",
      "total reward after 80 steps is 16.0\n",
      "total reward after 81 steps is 14.0\n",
      "total reward after 82 steps is 15.0\n",
      "total reward after 83 steps is 9.0\n",
      "total reward after 84 steps is 39.0\n",
      "total reward after 85 steps is 12.0\n",
      "total reward after 86 steps is 18.0\n",
      "total reward after 87 steps is 16.0\n",
      "total reward after 88 steps is 18.0\n",
      "total reward after 89 steps is 18.0\n",
      "total reward after 90 steps is 37.0\n",
      "total reward after 91 steps is 15.0\n",
      "total reward after 92 steps is 29.0\n",
      "total reward after 93 steps is 49.0\n",
      "total reward after 94 steps is 24.0\n",
      "total reward after 95 steps is 26.0\n",
      "total reward after 96 steps is 42.0\n",
      "total reward after 97 steps is 14.0\n",
      "total reward after 98 steps is 10.0\n",
      "total reward after 99 steps is 34.0\n",
      "total reward after 100 steps is 33.0\n",
      "total reward after 101 steps is 9.0\n",
      "total reward after 102 steps is 10.0\n",
      "total reward after 103 steps is 12.0\n",
      "total reward after 104 steps is 32.0\n",
      "total reward after 105 steps is 17.0\n",
      "total reward after 106 steps is 38.0\n",
      "total reward after 107 steps is 21.0\n",
      "total reward after 108 steps is 20.0\n",
      "total reward after 109 steps is 15.0\n",
      "total reward after 110 steps is 34.0\n",
      "total reward after 111 steps is 11.0\n",
      "total reward after 112 steps is 57.0\n",
      "total reward after 113 steps is 16.0\n",
      "total reward after 114 steps is 18.0\n",
      "total reward after 115 steps is 19.0\n",
      "total reward after 116 steps is 44.0\n",
      "total reward after 117 steps is 43.0\n",
      "total reward after 118 steps is 14.0\n",
      "total reward after 119 steps is 22.0\n",
      "total reward after 120 steps is 52.0\n",
      "total reward after 121 steps is 45.0\n",
      "total reward after 122 steps is 11.0\n",
      "total reward after 123 steps is 14.0\n",
      "total reward after 124 steps is 21.0\n",
      "total reward after 125 steps is 22.0\n",
      "total reward after 126 steps is 21.0\n",
      "total reward after 127 steps is 28.0\n",
      "total reward after 128 steps is 28.0\n",
      "total reward after 129 steps is 35.0\n",
      "total reward after 130 steps is 26.0\n",
      "total reward after 131 steps is 23.0\n",
      "total reward after 132 steps is 21.0\n",
      "total reward after 133 steps is 23.0\n",
      "total reward after 134 steps is 35.0\n",
      "total reward after 135 steps is 19.0\n",
      "total reward after 136 steps is 34.0\n",
      "total reward after 137 steps is 12.0\n",
      "total reward after 138 steps is 13.0\n",
      "total reward after 139 steps is 79.0\n",
      "total reward after 140 steps is 72.0\n",
      "total reward after 141 steps is 14.0\n",
      "total reward after 142 steps is 23.0\n",
      "total reward after 143 steps is 14.0\n",
      "total reward after 144 steps is 23.0\n",
      "total reward after 145 steps is 11.0\n",
      "total reward after 146 steps is 21.0\n",
      "total reward after 147 steps is 20.0\n",
      "total reward after 148 steps is 9.0\n",
      "total reward after 149 steps is 17.0\n",
      "total reward after 150 steps is 16.0\n",
      "total reward after 151 steps is 28.0\n",
      "total reward after 152 steps is 29.0\n",
      "total reward after 153 steps is 17.0\n",
      "total reward after 154 steps is 38.0\n",
      "total reward after 155 steps is 20.0\n",
      "total reward after 156 steps is 59.0\n",
      "total reward after 157 steps is 19.0\n",
      "total reward after 158 steps is 14.0\n",
      "total reward after 159 steps is 18.0\n",
      "total reward after 160 steps is 12.0\n",
      "total reward after 161 steps is 54.0\n",
      "total reward after 162 steps is 23.0\n",
      "total reward after 163 steps is 98.0\n",
      "total reward after 164 steps is 16.0\n",
      "total reward after 165 steps is 32.0\n",
      "total reward after 166 steps is 16.0\n",
      "total reward after 167 steps is 36.0\n",
      "total reward after 168 steps is 32.0\n",
      "total reward after 169 steps is 17.0\n",
      "total reward after 170 steps is 12.0\n",
      "total reward after 171 steps is 20.0\n",
      "total reward after 172 steps is 34.0\n",
      "total reward after 173 steps is 11.0\n",
      "total reward after 174 steps is 12.0\n",
      "total reward after 175 steps is 30.0\n",
      "total reward after 176 steps is 74.0\n",
      "total reward after 177 steps is 13.0\n",
      "total reward after 178 steps is 23.0\n",
      "total reward after 179 steps is 23.0\n",
      "total reward after 180 steps is 22.0\n",
      "total reward after 181 steps is 11.0\n",
      "total reward after 182 steps is 28.0\n",
      "total reward after 183 steps is 13.0\n",
      "total reward after 184 steps is 13.0\n",
      "total reward after 185 steps is 16.0\n",
      "total reward after 186 steps is 17.0\n",
      "total reward after 187 steps is 10.0\n",
      "total reward after 188 steps is 13.0\n",
      "total reward after 189 steps is 16.0\n",
      "total reward after 190 steps is 57.0\n",
      "total reward after 191 steps is 53.0\n",
      "total reward after 192 steps is 37.0\n",
      "total reward after 193 steps is 37.0\n",
      "total reward after 194 steps is 40.0\n",
      "total reward after 195 steps is 10.0\n",
      "total reward after 196 steps is 11.0\n",
      "total reward after 197 steps is 18.0\n",
      "total reward after 198 steps is 17.0\n",
      "total reward after 199 steps is 36.0\n",
      "total reward after 200 steps is 10.0\n",
      "total reward after 201 steps is 36.0\n",
      "total reward after 202 steps is 43.0\n",
      "total reward after 203 steps is 24.0\n",
      "total reward after 204 steps is 39.0\n",
      "total reward after 205 steps is 37.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 206 steps is 77.0\n",
      "total reward after 207 steps is 61.0\n",
      "total reward after 208 steps is 25.0\n",
      "total reward after 209 steps is 13.0\n",
      "total reward after 210 steps is 10.0\n",
      "total reward after 211 steps is 87.0\n",
      "total reward after 212 steps is 38.0\n",
      "total reward after 213 steps is 18.0\n",
      "total reward after 214 steps is 28.0\n",
      "total reward after 215 steps is 18.0\n",
      "total reward after 216 steps is 21.0\n",
      "total reward after 217 steps is 24.0\n",
      "total reward after 218 steps is 21.0\n",
      "total reward after 219 steps is 42.0\n",
      "total reward after 220 steps is 13.0\n",
      "total reward after 221 steps is 44.0\n",
      "total reward after 222 steps is 35.0\n",
      "total reward after 223 steps is 39.0\n",
      "total reward after 224 steps is 66.0\n",
      "total reward after 225 steps is 39.0\n",
      "total reward after 226 steps is 15.0\n",
      "total reward after 227 steps is 22.0\n",
      "total reward after 228 steps is 17.0\n",
      "total reward after 229 steps is 45.0\n",
      "total reward after 230 steps is 50.0\n",
      "total reward after 231 steps is 38.0\n",
      "total reward after 232 steps is 15.0\n",
      "total reward after 233 steps is 15.0\n",
      "total reward after 234 steps is 17.0\n",
      "total reward after 235 steps is 33.0\n",
      "total reward after 236 steps is 38.0\n",
      "total reward after 237 steps is 17.0\n",
      "total reward after 238 steps is 11.0\n",
      "total reward after 239 steps is 48.0\n",
      "total reward after 240 steps is 55.0\n",
      "total reward after 241 steps is 78.0\n",
      "total reward after 242 steps is 56.0\n",
      "total reward after 243 steps is 29.0\n",
      "total reward after 244 steps is 61.0\n",
      "total reward after 245 steps is 16.0\n",
      "total reward after 246 steps is 14.0\n",
      "total reward after 247 steps is 13.0\n",
      "total reward after 248 steps is 30.0\n",
      "total reward after 249 steps is 19.0\n",
      "total reward after 250 steps is 35.0\n",
      "total reward after 251 steps is 22.0\n",
      "total reward after 252 steps is 35.0\n",
      "total reward after 253 steps is 32.0\n",
      "total reward after 254 steps is 21.0\n",
      "total reward after 255 steps is 64.0\n",
      "total reward after 256 steps is 26.0\n",
      "total reward after 257 steps is 49.0\n",
      "total reward after 258 steps is 12.0\n",
      "total reward after 259 steps is 12.0\n",
      "total reward after 260 steps is 18.0\n",
      "total reward after 261 steps is 31.0\n",
      "total reward after 262 steps is 12.0\n",
      "total reward after 263 steps is 22.0\n",
      "total reward after 264 steps is 32.0\n",
      "total reward after 265 steps is 15.0\n",
      "total reward after 266 steps is 31.0\n",
      "total reward after 267 steps is 10.0\n",
      "total reward after 268 steps is 25.0\n",
      "total reward after 269 steps is 22.0\n",
      "total reward after 270 steps is 15.0\n",
      "total reward after 271 steps is 21.0\n",
      "total reward after 272 steps is 33.0\n",
      "total reward after 273 steps is 31.0\n",
      "total reward after 274 steps is 21.0\n",
      "total reward after 275 steps is 57.0\n",
      "total reward after 276 steps is 14.0\n",
      "total reward after 277 steps is 44.0\n",
      "total reward after 278 steps is 19.0\n",
      "total reward after 279 steps is 88.0\n",
      "total reward after 280 steps is 30.0\n",
      "total reward after 281 steps is 12.0\n",
      "total reward after 282 steps is 32.0\n",
      "total reward after 283 steps is 23.0\n",
      "total reward after 284 steps is 25.0\n",
      "total reward after 285 steps is 13.0\n",
      "total reward after 286 steps is 41.0\n",
      "total reward after 287 steps is 20.0\n",
      "total reward after 288 steps is 34.0\n",
      "total reward after 289 steps is 45.0\n",
      "total reward after 290 steps is 160.0\n",
      "total reward after 291 steps is 56.0\n",
      "total reward after 292 steps is 29.0\n",
      "total reward after 293 steps is 64.0\n",
      "total reward after 294 steps is 31.0\n",
      "total reward after 295 steps is 14.0\n",
      "total reward after 296 steps is 32.0\n",
      "total reward after 297 steps is 45.0\n",
      "total reward after 298 steps is 48.0\n",
      "total reward after 299 steps is 14.0\n",
      "total reward after 300 steps is 28.0\n",
      "total reward after 301 steps is 20.0\n",
      "total reward after 302 steps is 33.0\n",
      "total reward after 303 steps is 22.0\n",
      "total reward after 304 steps is 12.0\n",
      "total reward after 305 steps is 20.0\n",
      "total reward after 306 steps is 39.0\n",
      "total reward after 307 steps is 23.0\n",
      "total reward after 308 steps is 59.0\n",
      "total reward after 309 steps is 17.0\n",
      "total reward after 310 steps is 24.0\n",
      "total reward after 311 steps is 63.0\n",
      "total reward after 312 steps is 12.0\n",
      "total reward after 313 steps is 21.0\n",
      "total reward after 314 steps is 54.0\n",
      "total reward after 315 steps is 49.0\n",
      "total reward after 316 steps is 14.0\n",
      "total reward after 317 steps is 46.0\n",
      "total reward after 318 steps is 29.0\n",
      "total reward after 319 steps is 9.0\n",
      "total reward after 320 steps is 12.0\n",
      "total reward after 321 steps is 19.0\n",
      "total reward after 322 steps is 22.0\n",
      "total reward after 323 steps is 25.0\n",
      "total reward after 324 steps is 23.0\n",
      "total reward after 325 steps is 37.0\n",
      "total reward after 326 steps is 26.0\n",
      "total reward after 327 steps is 78.0\n",
      "total reward after 328 steps is 23.0\n",
      "total reward after 329 steps is 24.0\n",
      "total reward after 330 steps is 19.0\n",
      "total reward after 331 steps is 29.0\n",
      "total reward after 332 steps is 25.0\n",
      "total reward after 333 steps is 18.0\n",
      "total reward after 334 steps is 23.0\n",
      "total reward after 335 steps is 44.0\n",
      "total reward after 336 steps is 34.0\n",
      "total reward after 337 steps is 22.0\n",
      "total reward after 338 steps is 39.0\n",
      "total reward after 339 steps is 38.0\n",
      "total reward after 340 steps is 28.0\n",
      "total reward after 341 steps is 65.0\n",
      "total reward after 342 steps is 25.0\n",
      "total reward after 343 steps is 21.0\n",
      "total reward after 344 steps is 18.0\n",
      "total reward after 345 steps is 34.0\n",
      "total reward after 346 steps is 103.0\n",
      "total reward after 347 steps is 18.0\n",
      "total reward after 348 steps is 55.0\n",
      "total reward after 349 steps is 39.0\n",
      "total reward after 350 steps is 50.0\n",
      "total reward after 351 steps is 18.0\n",
      "total reward after 352 steps is 12.0\n",
      "total reward after 353 steps is 12.0\n",
      "total reward after 354 steps is 43.0\n",
      "total reward after 355 steps is 11.0\n",
      "total reward after 356 steps is 22.0\n",
      "total reward after 357 steps is 38.0\n",
      "total reward after 358 steps is 30.0\n",
      "total reward after 359 steps is 15.0\n",
      "total reward after 360 steps is 21.0\n",
      "total reward after 361 steps is 17.0\n",
      "total reward after 362 steps is 35.0\n",
      "total reward after 363 steps is 30.0\n",
      "total reward after 364 steps is 53.0\n",
      "total reward after 365 steps is 10.0\n",
      "total reward after 366 steps is 69.0\n",
      "total reward after 367 steps is 28.0\n",
      "total reward after 368 steps is 26.0\n",
      "total reward after 369 steps is 35.0\n",
      "total reward after 370 steps is 45.0\n",
      "total reward after 371 steps is 20.0\n",
      "total reward after 372 steps is 41.0\n",
      "total reward after 373 steps is 66.0\n",
      "total reward after 374 steps is 15.0\n",
      "total reward after 375 steps is 17.0\n",
      "total reward after 376 steps is 79.0\n",
      "total reward after 377 steps is 18.0\n",
      "total reward after 378 steps is 62.0\n",
      "total reward after 379 steps is 42.0\n",
      "total reward after 380 steps is 59.0\n",
      "total reward after 381 steps is 27.0\n",
      "total reward after 382 steps is 55.0\n",
      "total reward after 383 steps is 106.0\n",
      "total reward after 384 steps is 46.0\n",
      "total reward after 385 steps is 97.0\n",
      "total reward after 386 steps is 21.0\n",
      "total reward after 387 steps is 53.0\n",
      "total reward after 388 steps is 24.0\n",
      "total reward after 389 steps is 56.0\n",
      "total reward after 390 steps is 70.0\n",
      "total reward after 391 steps is 34.0\n",
      "total reward after 392 steps is 66.0\n",
      "total reward after 393 steps is 26.0\n",
      "total reward after 394 steps is 32.0\n",
      "total reward after 395 steps is 34.0\n",
      "total reward after 396 steps is 13.0\n",
      "total reward after 397 steps is 33.0\n",
      "total reward after 398 steps is 56.0\n",
      "total reward after 399 steps is 57.0\n",
      "total reward after 400 steps is 66.0\n",
      "total reward after 401 steps is 91.0\n",
      "total reward after 402 steps is 30.0\n",
      "total reward after 403 steps is 70.0\n",
      "total reward after 404 steps is 52.0\n",
      "total reward after 405 steps is 44.0\n",
      "total reward after 406 steps is 18.0\n",
      "total reward after 407 steps is 41.0\n",
      "total reward after 408 steps is 43.0\n",
      "total reward after 409 steps is 41.0\n",
      "total reward after 410 steps is 23.0\n",
      "total reward after 411 steps is 47.0\n",
      "total reward after 412 steps is 22.0\n",
      "total reward after 413 steps is 40.0\n",
      "total reward after 414 steps is 64.0\n",
      "total reward after 415 steps is 21.0\n",
      "total reward after 416 steps is 39.0\n",
      "total reward after 417 steps is 32.0\n",
      "total reward after 418 steps is 18.0\n",
      "total reward after 419 steps is 23.0\n",
      "total reward after 420 steps is 42.0\n",
      "total reward after 421 steps is 58.0\n",
      "total reward after 422 steps is 57.0\n",
      "total reward after 423 steps is 9.0\n",
      "total reward after 424 steps is 25.0\n",
      "total reward after 425 steps is 40.0\n",
      "total reward after 426 steps is 31.0\n",
      "total reward after 427 steps is 25.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 428 steps is 94.0\n",
      "total reward after 429 steps is 107.0\n",
      "total reward after 430 steps is 40.0\n",
      "total reward after 431 steps is 47.0\n",
      "total reward after 432 steps is 12.0\n",
      "total reward after 433 steps is 17.0\n",
      "total reward after 434 steps is 61.0\n",
      "total reward after 435 steps is 75.0\n",
      "total reward after 436 steps is 26.0\n",
      "total reward after 437 steps is 61.0\n",
      "total reward after 438 steps is 29.0\n",
      "total reward after 439 steps is 66.0\n",
      "total reward after 440 steps is 65.0\n",
      "total reward after 441 steps is 19.0\n",
      "total reward after 442 steps is 69.0\n",
      "total reward after 443 steps is 27.0\n",
      "total reward after 444 steps is 32.0\n",
      "total reward after 445 steps is 41.0\n",
      "total reward after 446 steps is 86.0\n",
      "total reward after 447 steps is 73.0\n",
      "total reward after 448 steps is 42.0\n",
      "total reward after 449 steps is 79.0\n",
      "total reward after 450 steps is 40.0\n",
      "total reward after 451 steps is 100.0\n",
      "total reward after 452 steps is 27.0\n",
      "total reward after 453 steps is 72.0\n",
      "total reward after 454 steps is 63.0\n",
      "total reward after 455 steps is 13.0\n",
      "total reward after 456 steps is 78.0\n",
      "total reward after 457 steps is 17.0\n",
      "total reward after 458 steps is 98.0\n",
      "total reward after 459 steps is 33.0\n",
      "total reward after 460 steps is 51.0\n",
      "total reward after 461 steps is 11.0\n",
      "total reward after 462 steps is 21.0\n",
      "total reward after 463 steps is 27.0\n",
      "total reward after 464 steps is 19.0\n",
      "total reward after 465 steps is 48.0\n",
      "total reward after 466 steps is 16.0\n",
      "total reward after 467 steps is 68.0\n",
      "total reward after 468 steps is 14.0\n",
      "total reward after 469 steps is 72.0\n",
      "total reward after 470 steps is 24.0\n",
      "total reward after 471 steps is 49.0\n",
      "total reward after 472 steps is 48.0\n",
      "total reward after 473 steps is 27.0\n",
      "total reward after 474 steps is 34.0\n",
      "total reward after 475 steps is 30.0\n",
      "total reward after 476 steps is 20.0\n",
      "total reward after 477 steps is 95.0\n",
      "total reward after 478 steps is 95.0\n",
      "total reward after 479 steps is 27.0\n",
      "total reward after 480 steps is 17.0\n",
      "total reward after 481 steps is 76.0\n",
      "total reward after 482 steps is 35.0\n",
      "total reward after 483 steps is 54.0\n",
      "total reward after 484 steps is 24.0\n",
      "total reward after 485 steps is 23.0\n",
      "total reward after 486 steps is 73.0\n",
      "total reward after 487 steps is 18.0\n",
      "total reward after 488 steps is 36.0\n",
      "total reward after 489 steps is 85.0\n",
      "total reward after 490 steps is 53.0\n",
      "total reward after 491 steps is 15.0\n",
      "total reward after 492 steps is 48.0\n",
      "total reward after 493 steps is 118.0\n",
      "total reward after 494 steps is 17.0\n",
      "total reward after 495 steps is 83.0\n",
      "total reward after 496 steps is 21.0\n",
      "total reward after 497 steps is 73.0\n",
      "total reward after 498 steps is 48.0\n",
      "total reward after 499 steps is 26.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agentoo7 = agent()\n",
    "steps = 500\n",
    "for s in range(steps):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = agentoo7.act(state)\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            agentoo7.train(states, rewards, actions)\n",
    "            #print(\"total step for this episord are {}\".format(t))\n",
    "            print(\"total reward after {} steps is {}\".format(s, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
